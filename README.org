#+TITLE: Haskelleene
#+TODO: T(t) | W(w) D(d)

Final project and report for Functional Programming Course. Implementation of deterministic finite automata, non-deterministic finite automata, and regular expressions within Haskell; in particular an algorithm for converting between automata and regular expressions, and vice versa.

* The Plan

** Core Structures

These are the core data types that we work with throughout the project. Much of the project is about the well-known equivalence between them, also known as Kleene's Theorem. 

*** AutData

A simple encoding of automaton data, with no restriction on if it is deterministic or not. Essentially for convenience when moving between the different structures.

*** DFAs

Comprised of a set of states, a subset of accepting states, and a function taking a letter and a state to a new state.

*** NFAs

Similar to a DFA, except it can take a letter to a set of possible output states, or also take nothing to transition from one state to another.

*** Regular Expressions

Built using the syntax of Kleene Algebra, used to specify regular expressions. Implemented similarly to propositional logic, but with an even nicer show function.

** Translating Between Structures

As stated above, a large part of the project is about being able to encode a regular language as any of a DFA, NFA, or regular expression, equivalently. These functions allow for translating from one of the data types to another in a way that preserves the language specified.

*** Creating an Automaton from Data (detCheck, encodeDA,encodeNA)

~detCheck~ is a function checking if a certain set of automaton data can actually specify a deterministic automaton (no empty transitions, transition function is deterministic at every state). If an data set can pass ~detCheck~, then we can encode it as a DFA. For this purpose we have the function ~encodeDA~, which produces an automaton if the data passes ~detCheck~, and ~Nothing~ otherwise.

Because all data can be turned into a valid NFA, we have no need for such a check, and so we have the function ~encodeNA~ for turning

Note that we don't check at any point that the accepting states are a subset of the all states; in such a scenario all "accepting states" not specified as valid states are simply ignored.

*** Power-set Determinisation (fromNA)

We use the well-known power-set construction to turn a NFA into a DFA.

*** Forgetting Determinism (fromDA)

Because all DFAs can also be considered NFAs (by just ignoring the determinism requirement), we allow DFAs to be "lifted" directly to NFAs.

*** Extracting Data from Automata (decode)

Our functions for translating between automata and data actually operate on automaton data, as it's much easier to handle tuples/lists than functions in such a setting. Therefore, we have the function ~decode~ to undo the work of encoding an automaton, by turning it into a data set.

This function operates on NFAs, so to decode a DFA, one can just compose this function with ~fromDA~ to turn it into an NFA.

*** Kleene's Algorithm (autToReg)

Kleene's Algorithm, closely related to the Floyd-Warshall algorithm for shorted paths in a graph, generates a regular expression given some input automaton.

*** Generating Automata (regToAut)

Generates an automaton to represent a regular expression.

** QuickCheck

*** T Encoding

Check if we encode deterministic data as both a DFA and NFA, it should accept the same words.

*** T Determinisation

If we determinise an NFA, its determinised version should accept the same words.

*** T Language Equivalence

An automaton and the regex it generates should accept the same words. Similarly, some regex and the automaton it generates should accept the same words.

*** T Mutual Inverses 

An automaton, and the output of converting it into a regex and back to an automaton should accept the same words. Similarly with a regex and converting it into an automaton, and back into a regex.

** T Possible Next Things

*** MSO
Monadic second order logic with successor is equivalent to regex, DFAs, and NFAs,.
There is an effective transformation between MSO formulas and automata, which we could implement.

*** Kleene Algebra with Hypotheses

Could implement hypothesis closure on automata and regular expression (in a subset of simple cases)

** T Report

A lot of the stuff in this readme can most likely be adapted into the report.

* Notes from template README

See [[report.pdf][report.pdf]] for documentation.

Note: To rename the project you need to replace all occurrences of "report" with your own library name in the following files:

- `hie.yaml`
- `package.yaml`

